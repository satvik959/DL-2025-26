{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeVgfNYl4NSYHul9L3jL7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik959/DL-2025-26/blob/main/week2DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQPkPslCxcov",
        "outputId": "8205e531-70b9-4daa-937c-dfc65d2e4b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0\n",
            "0 1 0\n",
            "1 0 0\n",
            "1 1 1\n",
            "0 0 0\n",
            "0 1 0\n",
            "1 0 0\n",
            "1 1 1\n"
          ]
        }
      ],
      "source": [
        "#1Q)Implement AND and OR logic operations using a single perceptron, and verify the correctness of the output using appropriate truth tables. (linear Data)\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    net = w1*x1 + w2*x2 + b\n",
        "    return 1 if net >= 0 else 0\n",
        "\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        print(x1, x2, perceptron(x1, x2, 1, 1, -1.5))\n",
        "\n",
        "\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        print(x1, x2, perceptron(x1, x2, 1, 1, -2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2Q)Examine the feasibility of implementing the XOR and XNOR (¬XOR) operations (Non linear data) using a single perceptron. If not possible, clearly explain the reason based on the concept of linear separability.\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_xor = np.array([0,1,1,0])\n",
        "y_xnor = np.array([1,0,0,1])\n",
        "\n",
        "def train(X, y, lr=0.1, epochs=50):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        updated = False\n",
        "        for xi, yi in zip(X, y):\n",
        "            y_pred = 1 if np.dot(xi, w) + b >= 0 else 0\n",
        "            if y_pred != yi:\n",
        "                w += lr * (yi - y_pred) * xi\n",
        "                b += lr * (yi - y_pred)\n",
        "                updated = True\n",
        "        if not updated:\n",
        "            return True, w, b\n",
        "    return False, w, b\n",
        "\n",
        "xor_result = train(X, y_xor)\n",
        "xnor_result = train(X, y_xnor)\n",
        "\n",
        "print(\"XOR converged:\", xor_result[0])\n",
        "print(\"XNOR converged:\", xnor_result[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp02cgNe2ufI",
        "outputId": "c5b68aca-f24e-4bbe-9f16-0553b8086347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR converged: False\n",
            "XNOR converged: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3Q)Implement the XOR and (¬XOR) logic operation using a multi-perceptron network, and analyze how multiple perceptrons overcome the limitations of a single perceptron.\n",
        "import numpy as np\n",
        "\n",
        "step = lambda x: (x >= 0).astype(int)\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "OR  = step(X @ [1,1] - 0.5)\n",
        "AND = step(X @ [1,1] - 1.5)\n",
        "\n",
        "XOR  = step(OR - AND - 0.5)\n",
        "XNOR = step(AND - OR + 0.5)\n",
        "\n",
        "print(XOR)\n",
        "print(XNOR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaHu1a8v6O5t",
        "outputId": "f4aeb2c0-c117-4e20-f13e-ee1f0b4744c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 0]\n",
            "[1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Perceptron Learning Algorithm and study the effect of weight updates on convergence for a binary decision problem such as determining whether a user would like to watch a movie.\n",
        "Note: Consider a small dataset(design your own excel csv sheet) of movie records with Boolean or real-valued features, for example:\n",
        " f1​: Is actor Matt Damon present\n",
        " f2​: Is the genre Thriller\n",
        " f3​: Is the director Christopher Nolan\n",
        " f4: IMDb rating (scaled between 0 and 1)\n",
        "The output label represents like (1) or dislike (0). Train the perceptron model using these features and observe how weight updates influence convergence and classification performance.\n",
        "Test with a sample record to show whether a perceptron properly classifies it or not.\n",
        "i) Check with MP Perceptron  (without weights and bias)\n"
      ],
      "metadata": {
        "id": "UXXBouT0_Wrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "X = np.array([\n",
        "    [1, 1, 0, 0.8],\n",
        "    [0, 1, 1, 0.9],\n",
        "    [1, 0, 0, 0.6],\n",
        "    [0, 0, 1, 0.7],\n",
        "    [1, 1, 1, 0.9]\n",
        "])\n",
        "y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "lr = 0.1\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    for i in range(len(X)):\n",
        "        y_pred = step(np.dot(X[i], w))\n",
        "        w = w + lr * (y[i] - y_pred) * X[i]\n",
        "\n",
        "print(\"Final weights:\", w)\n",
        "\n",
        "test = np.array([1, 1, 0, 0.85])\n",
        "result = step(np.dot(test, w))\n",
        "print(\"Prediction (1=Like, 0=Dislike):\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQU5Nc_c-5Q1",
        "outputId": "957c447c-56a8-4932-ceae-a224182a2dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: [ 0.    0.2   0.   -0.08]\n",
            "Prediction (1=Like, 0=Dislike): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([\n",
        "    [1, 1, 0, 0.8],\n",
        "    [0, 1, 1, 0.9],\n",
        "    [1, 0, 0, 0.6],\n",
        "    [0, 0, 1, 0.7],\n",
        "    [1, 1, 1, 0.9]\n",
        "])\n",
        "y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "def perceptron(inputs):\n",
        "    total = sum(inputs)\n",
        "    return 1 if total >= 0 else 0\n",
        "\n",
        "for i in range(len(X)):\n",
        "    pred = perceptron(X[i])\n",
        "    print(f\"Movie {i+1}, Prediction: {pred}, Actual: {y[i]}\")\n",
        "\n",
        "test = [1, 1, 0, 0.85]\n",
        "print(\"Test movie prediction:\", perceptron(test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU9NOUm_BRYN",
        "outputId": "ce4805f6-f6fc-4424-d54a-a8e870c8d7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie 1, Prediction: 1, Actual: 1\n",
            "Movie 2, Prediction: 1, Actual: 1\n",
            "Movie 3, Prediction: 1, Actual: 0\n",
            "Movie 4, Prediction: 1, Actual: 0\n",
            "Movie 5, Prediction: 1, Actual: 1\n",
            "Test movie prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X =[\n",
        "    [0,0,1,-1.5],\n",
        "    [0,1,1,-2],\n",
        "\n",
        "]\n",
        "\n",
        "def perceptron(x1,x2,w1,w2,b):\n",
        "  score = x1*w1+x2*w2+b\n",
        "  return 1 if score>=0 else 0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "p62tJj-lEC4s",
        "outputId": "a07ddcf7-3c1f-4594-b9ba-b2487a66c2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-2013581408.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2013581408.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def perceptron(x1,x2,w1,w2,b):\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Demonstration of Representation Power of a Perceptron Network\n",
        "Covers:\n",
        "(a) Number of Boolean functions for two inputs\n",
        "(b) Linear separability check\n",
        "(c) Single perceptron learning test\n",
        "(d) Growth analysis of non-linearly separable Boolean functions\n",
        "\"\"\"\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# (a) Number of Boolean functions\n",
        "def number_of_boolean_functions(n):\n",
        "    return 2 ** (2 ** n)\n",
        "\n",
        "print(\"Number of Boolean functions for 2 inputs:\",\n",
        "      number_of_boolean_functions(2))\n",
        "\n",
        "\n",
        "# Generate all Boolean functions for n binary inputs\n",
        "def generate_boolean_functions(n):\n",
        "    inputs = list(itertools.product([0, 1], repeat=n))\n",
        "    outputs = list(itertools.product([0, 1], repeat=len(inputs)))\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "# Perceptron implementation\n",
        "class Perceptron:\n",
        "    def __init__(self, lr=0.1, epochs=100):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            error = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                y_pred = self.predict(xi)\n",
        "                update = self.lr * (target - y_pred)\n",
        "                self.w += update * xi\n",
        "                self.b += update\n",
        "                error += abs(target - y_pred)\n",
        "            if error == 0:\n",
        "                return True   # linearly separable\n",
        "        return False          # not linearly separable\n",
        "\n",
        "    def predict(self, x):\n",
        "        return 1 if np.dot(x, self.w) + self.b >= 0 else 0\n",
        "\n",
        "\n",
        "# (b) Check linear separability for each Boolean fn\n",
        "def check_linear_separability(n):\n",
        "    inputs, functions = generate_boolean_functions(n)\n",
        "    X = np.array(inputs)\n",
        "\n",
        "    separable = []\n",
        "    non_separable = []\n",
        "\n",
        "    for idx, f in enumerate(functions, 1):\n",
        "        p = Perceptron()\n",
        "        success = p.fit(X, np.array(f))\n",
        "        if success:\n",
        "            separable.append(idx)\n",
        "        else:\n",
        "            non_separable.append(idx)\n",
        "\n",
        "    return separable, non_separable\n",
        "\n",
        "\n",
        "sep, non_sep = check_linear_separability(2)\n",
        "\n",
        "print(\"\\nLinearly separable Boolean functions (2 inputs):\", sep)\n",
        "print(\"Non-linearly separable Boolean functions (2 inputs):\", non_sep)\n",
        "print(\"Count:\")\n",
        "print(\"  Separable:\", len(sep))\n",
        "print(\"  Non-separable:\", len(non_sep))\n",
        "\n",
        "\n",
        "# (c) Explicit perceptron learning test for all functions\n",
        "print(\"\\nPerceptron learning results:\")\n",
        "for i in range(1, 17):\n",
        "    status = \"Learned\" if i in sep else \"Not Learned\"\n",
        "    print(f\"Function f{i}: {status}\")\n",
        "\n",
        "print(\"\\nReason perceptron fails:\")\n",
        "print(\"A single perceptron cannot learn non-linearly separable functions\")\n",
        "print(\"such as XOR and XNOR due to absence of a linear decision boundary.\")\n",
        "\n",
        "\n",
        "# (d) Growth of non-linearly separable functions vs n\n",
        "def analyze_growth(max_n=4):\n",
        "    print(\"\\nGrowth of Non-Linearly Separable Boolean Functions:\")\n",
        "    print(\"n | Total Functions | Linearly Separable | Non-Linearly Separable\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for n in range(1, max_n + 1):\n",
        "        total = number_of_boolean_functions(n)\n",
        "        if n <= 3:\n",
        "            sep, non_sep = check_linear_separability(n)\n",
        "            linear = len(sep)\n",
        "            non_linear = len(non_sep)\n",
        "        else:\n",
        "            # Known results / estimates for higher n\n",
        "            linear = \"Very Small\"\n",
        "            non_linear = \"Almost All\"\n",
        "\n",
        "        print(f\"{n} | {total:<15} | {linear:<18} | {non_linear}\")\n",
        "\n",
        "analyze_growth(max_n=4)\n"
      ],
      "metadata": {
        "id": "-AC2YkA3FxCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b859b303-a451-4d57-9a17-ea6987d375a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Boolean functions for 2 inputs: 16\n",
            "\n",
            "Linearly separable Boolean functions (2 inputs): [1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16]\n",
            "Non-linearly separable Boolean functions (2 inputs): [7, 10]\n",
            "Count:\n",
            "  Separable: 14\n",
            "  Non-separable: 2\n",
            "\n",
            "Perceptron learning results:\n",
            "Function f1: Learned\n",
            "Function f2: Learned\n",
            "Function f3: Learned\n",
            "Function f4: Learned\n",
            "Function f5: Learned\n",
            "Function f6: Learned\n",
            "Function f7: Not Learned\n",
            "Function f8: Learned\n",
            "Function f9: Learned\n",
            "Function f10: Not Learned\n",
            "Function f11: Learned\n",
            "Function f12: Learned\n",
            "Function f13: Learned\n",
            "Function f14: Learned\n",
            "Function f15: Learned\n",
            "Function f16: Learned\n",
            "\n",
            "Reason perceptron fails:\n",
            "A single perceptron cannot learn non-linearly separable functions\n",
            "such as XOR and XNOR due to absence of a linear decision boundary.\n",
            "\n",
            "Growth of Non-Linearly Separable Boolean Functions:\n",
            "n | Total Functions | Linearly Separable | Non-Linearly Separable\n",
            "-----------------------------------------------------------------\n",
            "1 | 4               | 4                  | 0\n",
            "2 | 16              | 14                 | 2\n",
            "3 | 256             | 104                | 152\n",
            "4 | 65536           | Very Small         | Almost All\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q7. Design and implement a Multi-Layer Perceptron (MLP)\n",
        "capable of realizing ALL Boolean functions of two inputs.\n",
        "\n",
        "Architecture (fixed, as in the figure / hint):\n",
        "\n",
        "Inputs: x1, x2\n",
        "Hidden layer: 4 perceptrons (h1, h2, h3, h4)\n",
        "Output layer: 1 perceptron (y)\n",
        "\n",
        "Key idea:\n",
        "- Each hidden neuron fires for exactly ONE input pattern\n",
        "- Output neuron combines hidden activations to realize ANY Boolean function\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# Step 1: Basic Perceptron Unit\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.w = np.array(weights)\n",
        "        self.b = bias\n",
        "\n",
        "    def activate(self, x):\n",
        "        return 1 if np.dot(self.w, x) + self.b >= 0 else 0\n",
        "\n",
        "\n",
        "# Step 2: Hidden layer construction\n",
        "# Each neuron detects ONE input pattern\n",
        "# Input patterns in fixed order:\n",
        "# (0,0), (0,1), (1,0), (1,1)\n",
        "\n",
        "hidden_neurons = {\n",
        "    (0, 0): Perceptron(weights=[-1, -1], bias=0),\n",
        "    (0, 1): Perceptron(weights=[-1,  1], bias=0),\n",
        "    (1, 0): Perceptron(weights=[ 1, -1], bias=0),\n",
        "    (1, 1): Perceptron(weights=[ 1,  1], bias=-1),\n",
        "}\n",
        "\n",
        "# Step 3: MLP Model\n",
        "class BooleanMLP:\n",
        "    def __init__(self, truth_table):\n",
        "        \"\"\"\n",
        "        truth_table: list of 4 outputs for\n",
        "        [(0,0), (0,1), (1,0), (1,1)]\n",
        "        \"\"\"\n",
        "        self.truth_table = truth_table\n",
        "        self.hidden = hidden_neurons\n",
        "\n",
        "        # Output neuron weights = truth table itself\n",
        "        self.output_weights = np.array(truth_table)\n",
        "        self.output_bias = -0.5  # fires if any required hidden fires\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_out = []\n",
        "        for pattern in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "            h_out.append(self.hidden[pattern].activate(x))\n",
        "        h_out = np.array(h_out)\n",
        "\n",
        "        y = np.dot(self.output_weights, h_out) + self.output_bias\n",
        "        return 1 if y >= 0 else 0\n",
        "\n",
        "\n",
        "# Step 4: Test ALL Boolean functions\n",
        "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "all_boolean_functions = list(itertools.product([0,1], repeat=4))\n",
        "\n",
        "success = True\n",
        "\n",
        "for idx, func in enumerate(all_boolean_functions, 1):\n",
        "    mlp = BooleanMLP(func)\n",
        "    for x, expected in zip(inputs, func):\n",
        "        output = mlp.forward(x)\n",
        "        if output != expected:\n",
        "            success = False\n",
        "            print(f\"Function f{idx} FAILED for input {x}\")\n",
        "            break\n",
        "\n",
        "print(\"\\nResult:\")\n",
        "if success:\n",
        "    print(\"✅ MLP successfully represents ALL 16 Boolean functions\")\n",
        "else:\n",
        "    print(\"❌ Some Boolean functions failed\")\n",
        "\n",
        "\n",
        "# Step 5: Explanation Summary (printed)\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"\"\"\n",
        "• Hidden layer has 4 perceptrons\n",
        "• Each hidden perceptron activates for exactly ONE input\n",
        "• This creates a one-hot encoding of the input space\n",
        "• Output perceptron linearly combines hidden activations\n",
        "• Hence ANY Boolean function can be represented\n",
        "• This overcomes the XOR limitation of a single perceptron\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVFE4g6s_wgW",
        "outputId": "da181d72-e9f6-4628-e135-660b633d6a51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function f2 FAILED for input (0, 1)\n",
            "Function f3 FAILED for input (0, 0)\n",
            "Function f4 FAILED for input (0, 0)\n",
            "Function f5 FAILED for input (0, 0)\n",
            "Function f6 FAILED for input (0, 0)\n",
            "Function f7 FAILED for input (0, 0)\n",
            "Function f8 FAILED for input (0, 0)\n",
            "Function f10 FAILED for input (0, 1)\n",
            "Function f11 FAILED for input (1, 1)\n",
            "Function f12 FAILED for input (0, 1)\n",
            "Function f13 FAILED for input (1, 1)\n",
            "Function f14 FAILED for input (1, 0)\n",
            "Function f15 FAILED for input (1, 1)\n",
            "\n",
            "Result:\n",
            "❌ Some Boolean functions failed\n",
            "\n",
            "Explanation:\n",
            "\n",
            "• Hidden layer has 4 perceptrons\n",
            "• Each hidden perceptron activates for exactly ONE input\n",
            "• This creates a one-hot encoding of the input space\n",
            "• Output perceptron linearly combines hidden activations\n",
            "• Hence ANY Boolean function can be represented\n",
            "• This overcomes the XOR limitation of a single perceptron\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def mlp_boolean(x, hidden_weights, hidden_biases, out_weights, out_bias):\n",
        "    hidden_outputs = []\n",
        "    for w, b in zip(hidden_weights, hidden_biases):\n",
        "        hidden_outputs.append(step(np.dot(w, x) + b))\n",
        "    return step(np.dot(out_weights, hidden_outputs) + out_bias)\n",
        "\n",
        "def build_mlp(n, boolean_function):\n",
        "    inputs = list(product([0,1], repeat=n))\n",
        "\n",
        "    hidden_weights = []\n",
        "    hidden_biases = []\n",
        "\n",
        "    for inp in inputs:\n",
        "        w = [1 if bit == 1 else -1 for bit in inp]\n",
        "        b = -sum(1 for bit in inp if bit == 1) + 0.5\n",
        "        hidden_weights.append(np.array(w))\n",
        "        hidden_biases.append(b)\n",
        "\n",
        "    out_weights = np.array(boolean_function)\n",
        "    out_bias = -0.5\n",
        "\n",
        "    return hidden_weights, hidden_biases, out_weights, out_bias\n",
        "\n",
        "boolean_function = [0,1,1,0]  # XOR\n",
        "hw, hb, ow, ob = build_mlp(2, boolean_function)\n",
        "\n",
        "for x in product([0,1], repeat=2):\n",
        "    print(x, mlp_boolean(np.array(x), hw, hb, ow, ob))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-gAU_gy_0wN",
        "outputId": "d902809f-7684-4d97-9736-0e8e3d840ef6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0) 0\n",
            "(0, 1) 1\n",
            "(1, 0) 1\n",
            "(1, 1) 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import numpy as np\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "X = list(product([0,1], repeat=3))\n",
        "\n",
        "# Hidden layer (fixed)\n",
        "hidden_weights = []\n",
        "hidden_biases = []\n",
        "\n",
        "for p in X:\n",
        "    w = np.array([1 if bit else -1 for bit in p])\n",
        "    b = -sum(p) + 0.5\n",
        "    hidden_weights.append(w)\n",
        "    hidden_biases.append(b)\n",
        "\n",
        "def predict(x, out_w, out_b):\n",
        "    h = [step(np.dot(w, x) + b)\n",
        "         for w, b in zip(hidden_weights, hidden_biases)]\n",
        "    return step(np.dot(out_w, h) + out_b)\n",
        "\n",
        "# Demonstrate selected Boolean functions\n",
        "examples = {\n",
        "    \"AND\":       [0,0,0,0,0,0,0,1],\n",
        "    \"OR\":        [0,1,1,1,1,1,1,1],\n",
        "    \"XOR\":       [0,1,1,0,1,0,0,1],\n",
        "    \"CONST_1\":   [1,1,1,1,1,1,1,1]\n",
        "}\n",
        "\n",
        "for name, f in examples.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    for x, y in zip(X, f):\n",
        "        print(x, \"→\", predict(np.array(x), f, -0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgmVwqGp_3ul",
        "outputId": "b55d4f01-b6b2-478d-b32e-10b938bd40b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AND\n",
            "(0, 0, 0) → 0\n",
            "(0, 0, 1) → 0\n",
            "(0, 1, 0) → 0\n",
            "(0, 1, 1) → 0\n",
            "(1, 0, 0) → 0\n",
            "(1, 0, 1) → 0\n",
            "(1, 1, 0) → 0\n",
            "(1, 1, 1) → 1\n",
            "\n",
            "OR\n",
            "(0, 0, 0) → 0\n",
            "(0, 0, 1) → 1\n",
            "(0, 1, 0) → 1\n",
            "(0, 1, 1) → 1\n",
            "(1, 0, 0) → 1\n",
            "(1, 0, 1) → 1\n",
            "(1, 1, 0) → 1\n",
            "(1, 1, 1) → 1\n",
            "\n",
            "XOR\n",
            "(0, 0, 0) → 0\n",
            "(0, 0, 1) → 1\n",
            "(0, 1, 0) → 1\n",
            "(0, 1, 1) → 0\n",
            "(1, 0, 0) → 1\n",
            "(1, 0, 1) → 0\n",
            "(1, 1, 0) → 0\n",
            "(1, 1, 1) → 1\n",
            "\n",
            "CONST_1\n",
            "(0, 0, 0) → 1\n",
            "(0, 0, 1) → 1\n",
            "(0, 1, 0) → 1\n",
            "(0, 1, 1) → 1\n",
            "(1, 0, 0) → 1\n",
            "(1, 0, 1) → 1\n",
            "(1, 1, 0) → 1\n",
            "(1, 1, 1) → 1\n"
          ]
        }
      ]
    }
  ]
}